{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# creat the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# creating by processing a string of text with the help of nlp object\n",
    "Doc= nlp(\"Hello World!\")\n",
    "\n",
    "# Iterate over token in a Doc\n",
    "for token in Doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing a doc object\n",
    "token = Doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "token.text # string type object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A slice from a Doc object is a span object\n",
    "Doc= nlp(\"Hello World!\")\n",
    "span = Doc[1:4]\n",
    "\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n",
      "Text:  ['It', 'cost', '$', '5', '.']\n",
      "Is_alpha:  [True, True, False, False, False]\n",
      "Is_punct:  [False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "Doc= nlp(\"It cost $5.\")\n",
    "\n",
    "print(\"Index: \", [token.i for token in Doc])\n",
    "print(\"Text: \",[token.text for token in Doc])\n",
    "print(\"Is_alpha: \",[token.is_alpha for token in Doc])\n",
    "print(\"Is_punct: \",[token.is_punct for token in Doc])\n",
    "print(\"like_num: \",[token.like_num for token in Doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load spacy model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting part-of-speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"he ate the pizza\")\n",
    "\n",
    "#Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and predict their part of speech.\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Syntactic Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#Process a text\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "#Iterate over predicted entities\n",
    "for token in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(token.text, token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# laod the model and create nlp object.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initial the matcher with shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the Matcher\n",
    "pattern = [{\"TEXT\":\"iPhone\"}, {\"TEXT\":\"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\",None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the match span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "custom_patter_1 = [\n",
    "    {'IS_DIGIT':True},\n",
    "    {'LOWER':'fifa'},\n",
    "    {'LOWER':'world'},\n",
    "    {'LOWER':'cup'},\n",
    "    {'IS_PUNCT':True}\n",
    "]\n",
    "\n",
    "matcher.add(\"cust_matcher_1\",None,custom_patter_1)\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the match span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Other Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "custom_patter_2 = [\n",
    "    {'LEMMA':'love', 'POS':'VERB'},\n",
    "    {'POS':'NOUN'}\n",
    "]\n",
    "\n",
    "matcher.add(\"cust_matcher_2\",None,custom_patter_2)\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the match span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Operators and Quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "custom_patter_3 = [\n",
    "    {'LEMMA':'buy'},\n",
    "    {'POS':\"DET\", 'OP':'?'}, #optional match 0 or 1 times.\n",
    "    {'POS':'NOUN'}\n",
    "]\n",
    "\n",
    "matcher.add(\"cust_matcher_2\",None,custom_patter_3)\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the match span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
