{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and updating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How training works\n",
    "\n",
    "1. Initialize the model weights randomly with nlp.begin_training\n",
    "2. Predict a few examples with the current weights by calling nlp.update\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weights slightly\n",
    "6. Go back to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]}) ('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]}) ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]}) ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]}) ('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}) ('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "# how to semi-automate to create training examples using Matcher in spacy\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "print(*TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loop for a number of times.\n",
    "2. Shuffle the training data.\n",
    "3. Divide the data into batches.\n",
    "4. Update the model for each batch.\n",
    "5. Save the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples...\n",
    "]\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "# loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create a btach and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # split the batch in text and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk('model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a training loop from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 8.000000059604645}\n",
      "{'ner': 22.496776401996613}\n",
      "{'ner': 31.853319942951202}\n",
      "{'ner': 7.109317302703857}\n",
      "{'ner': 13.006432175636292}\n",
      "{'ner': 18.542052090168}\n",
      "{'ner': 3.4365335404872894}\n",
      "{'ner': 6.569849184714258}\n",
      "{'ner': 7.88712777395267}\n",
      "{'ner': 2.3865505084686447}\n",
      "{'ner': 4.474786773978849}\n",
      "{'ner': 5.275617860221246}\n",
      "{'ner': 4.410458291764371}\n",
      "{'ner': 9.060808309703134}\n",
      "{'ner': 10.982691241573775}\n",
      "{'ner': 2.273387190653011}\n",
      "{'ner': 3.025673861593532}\n",
      "{'ner': 5.262153170981037}\n",
      "{'ner': 1.228482561185956}\n",
      "{'ner': 2.1085004426713567}\n",
      "{'ner': 2.769125703634927}\n",
      "{'ner': 0.23909635161635379}\n",
      "{'ner': 0.26650693952467464}\n",
      "{'ner': 1.2055368919477019}\n",
      "{'ner': 0.004890342182520158}\n",
      "{'ner': 0.013082372427990485}\n",
      "{'ner': 2.3215074471822303}\n",
      "{'ner': 2.1328314491493607}\n",
      "{'ner': 2.1330513375847806}\n",
      "{'ner': 2.133055827927847}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"gadgets.json\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "    \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices for training spaCy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"There's also a Paris in Arkansas, lol\",\n",
    "        {\"entities\": [(15, 20, \"GPE\"), (24, 32, \"GPE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"GPE\")]},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"PewDiePie smashes YouTube record\",\n",
    "        {\"entities\": [(0, 9, \"PERSON\"), (18, 25, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (15, 29, \"PERSON\")]},\n",
    "    ),\n",
    "]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
